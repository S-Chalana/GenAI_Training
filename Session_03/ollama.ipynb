{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b849b91",
   "metadata": {},
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a7b6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from ollama) (2.12.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\schal\\documents\\saruchi\\gen_ai_training\\.venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.6.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b774d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def ask_question_local_llm(prompt):\n",
    "    print(f\"user asked {prompt}\")\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assitant - Respond in one line\"},\n",
    "            {\"role\": \"user\", \"content\": prompt} \n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9968b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user asked What is the capital of France?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ask_question_local_llm(\"What is the capital of France?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae4d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------Local LLM Response------------------------- \n",
      "user asked what is the best ollama model for windows\n",
      "Time taken by Local LLM: 33.05658578872681 seconds\n",
      "\n",
      "Local LLM says: The best Ollama model for Windows is the Ollama V4.0, which offers advanced features such as real-time object detection, improved tracking capabilities, and compatibility with a wide range of Windows versions.\n",
      "\n",
      "\n",
      "-------------------------Local LLM Response------------------------- \n",
      "user asked what is ollama good for\n",
      "Time taken by Local LLM: 34.03151750564575 seconds\n",
      "\n",
      "Local LLM says: Ollama is a free online meeting and collaboration tool that allows users to host virtual meetings, share screens, and collaborate on projects with up to 12 participants simultaneously, making it an excellent choice for remote teams, students, and freelancers.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    user_prompt = input(\"Ask me anything\")\n",
    "    if (user_prompt.lower() != 'quit'):\n",
    "        print(\"\\n\\n-------------------------Local LLM Response------------------------- \")\n",
    "        start = time.time()\n",
    "        response_local= ask_question_local_llm(user_prompt)\n",
    "        end = time.time()\n",
    "        print(f\"Time taken by Local LLM: {end - start} seconds\")\n",
    "        print(\"\\nLocal LLM says:\", response_local)  \n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "    else:\n",
    "       print(\"Exiting...\")\n",
    "       break    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2f6f9",
   "metadata": {},
   "source": [
    "## Local LLM vs OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf65c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def ask_question_local_llm(prompt):\n",
    "    print(f\"user asked {prompt}\")\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assitant - Respond in one line\"},\n",
    "            {\"role\": \"user\", \"content\": prompt} \n",
    "        ]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
