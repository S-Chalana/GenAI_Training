{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608ab2ec",
   "metadata": {},
   "source": [
    "## Part 3: Build a Simple RAG System\n",
    "Objective: Create a minimal RAG pipeline using Milvus as the vector database.\n",
    "Load 3â€“5 small text files (or web paragraphs) as your dataset.\n",
    "Generate embeddings using SentenceTransformer(\"all-MiniLM-L6-v2\").\n",
    "Store them in Milvus.\n",
    "Write a query function that takes a user question, retrieves the top 3 most similar chunks, and prints them.\n",
    "Pass the retrieved context to an OpenAI model (or a local LLM) and generate an answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0854827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 text files.\n"
     ]
    }
   ],
   "source": [
    "## 1.Read .txt files from a directory\n",
    "TXT_DIR = \"documents\"\n",
    "def load_txt_files(txt_dir):\n",
    "    import os\n",
    "    texts = []\n",
    "    for filename in os.listdir(txt_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(txt_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "texts = load_txt_files(TXT_DIR)\n",
    "print(f\"Loaded {len(texts)} text files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ab4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##   2. Chunk text into smaller pieces\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6143c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schal\\Documents\\Saruchi\\Gen_AI_Training\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 3. prepare data for embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "MILVUS_HOST = \"127.0.0.1\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL)\n",
    "def embed_texts(texts):\n",
    "    embd = embed_model.encode(texts, convert_to_numpy=True)\n",
    "    return embd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9280c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  4. Connect to Milvus and create collection\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection   \n",
    "connections.connect(\"default\", host=\"127.0.0.1\", port=\"19530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ee4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Define schema and create collection\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "]\n",
    "schema = CollectionSchema(fields, \"Text embedding collection\")\n",
    "collection = Collection(\"text_embedding_collection\", schema)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20af3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Insert data into Milvus\n",
    "for text in texts:\n",
    "    chunks = chunk_text(text)\n",
    "    embeddings = embed_texts(chunks)\n",
    "    entities = [\n",
    "        embeddings.tolist(),\n",
    "        chunks\n",
    "    ]\n",
    "    collection.insert(entities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e45430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and collection loaded\n"
     ]
    }
   ],
   "source": [
    "## 7. create index and load collection to memory\n",
    "index_params = {    \n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"params\": {\"nlist\": 128},\n",
    "    \"metric_type\": \"L2\"\n",
    "}\n",
    "collection.create_index(\"embedding\", index_params)\n",
    "collection.load() \n",
    "print(\"Index created and collection loaded\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc64125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.  Retrieve similar texts\n",
    "def search_similar_texts(query, top_k=5):\n",
    "    query_embedding = embed_texts([query])[0].tolist()\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    results = collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=top_k,\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    similar_texts = [hit.entity.get(\"text\") for hit in results[0]]\n",
    "    return similar_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7be8d0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 similar chunks:\n",
      "1. ccording to the report.\n",
      "\n",
      "Amazon stock inched higher by 0.3% in Fridayâ€™s premarket.\n",
      "\n",
      "Get updates to this developing story directly on Stocktwits.\n",
      "2. AI bubble won't burst for one or two years: Kirk Yang\n",
      "Kirk Yang, Adjunct Finance Professor at National Taiwan University, says that he expect a strong correction in the AI markets in the next one to two years. He explains why he expects strong AI companies to survive the bubble burst, similar to the tech giants that surfaced after the Dot-com era.\n",
      "3. Why Gen Z Graduates Are Facing a Crisis Explained\n",
      "The curious minds at ColdFusion explain why Gen Z graduates are facing a crisis. This sheds light on structural challenges affecting employment, debt, and life stability.\n"
     ]
    }
   ],
   "source": [
    "## 9. Example query\n",
    "query = \"How is the market?\"\n",
    "results = search_similar_texts(query, top_k=3)\n",
    "print(\"\\nTop 3 similar chunks:\")\n",
    "for i, text in enumerate(results):\n",
    "    print(f\"{i+1}. {text}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f158847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "## 10. Setup OpenAI client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2935683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined: answer_query_with_context()\n"
     ]
    }
   ],
   "source": [
    "## 11. Function to pass retrieved context to OpenAI\n",
    "def answer_query_with_context(query, top_k=3, model=\"gpt-4o-mini\"):\n",
    "    # Retrieve similar texts from Milvus\n",
    "    retrieved_texts = search_similar_texts(query, top_k=top_k)\n",
    "    \n",
    "    # Combine retrieved texts as context\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Create prompt with context\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the following context extracted from documents to answer the user's question.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Call OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_completion_tokens=300,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    return answer, retrieved_texts\n",
    "\n",
    "print(\"Function defined: answer_query_with_context()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e943c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How is the market?\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“„ Retrieved Context (Top 3 chunks):\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. ccording to the report.\n",
      "\n",
      "Amazon stock inched higher by 0.3% in Fridayâ€™s premarket.\n",
      "\n",
      "Get updates to this developing story directly on Stocktwits.\n",
      "\n",
      "2. AI bubble won't burst for one or two years: Kirk Yang\n",
      "Kirk Yang, Adjunct Finance Professor at National Taiwan University, says that he expect a strong correction in the AI markets in the next one to t...\n",
      "\n",
      "3. Why Gen Z Graduates Are Facing a Crisis Explained\n",
      "The curious minds at ColdFusion explain why Gen Z graduates are facing a crisis. This sheds light on structural challenges affecting employment, debt,...\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ¤– AI Answer:\n",
      "------------------------------------------------------------\n",
      "The market is showing a slight increase, with Amazon stock inching higher by 0.3% in Fridayâ€™s premarket. However, there are concerns about a potential correction in the AI markets within the next one to two years, as noted by Kirk Yang.\n",
      "\n",
      "ðŸ“„ Retrieved Context (Top 3 chunks):\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. ccording to the report.\n",
      "\n",
      "Amazon stock inched higher by 0.3% in Fridayâ€™s premarket.\n",
      "\n",
      "Get updates to this developing story directly on Stocktwits.\n",
      "\n",
      "2. AI bubble won't burst for one or two years: Kirk Yang\n",
      "Kirk Yang, Adjunct Finance Professor at National Taiwan University, says that he expect a strong correction in the AI markets in the next one to t...\n",
      "\n",
      "3. Why Gen Z Graduates Are Facing a Crisis Explained\n",
      "The curious minds at ColdFusion explain why Gen Z graduates are facing a crisis. This sheds light on structural challenges affecting employment, debt,...\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ¤– AI Answer:\n",
      "------------------------------------------------------------\n",
      "The market is showing a slight increase, with Amazon stock inching higher by 0.3% in Fridayâ€™s premarket. However, there are concerns about a potential correction in the AI markets within the next one to two years, as noted by Kirk Yang.\n"
     ]
    }
   ],
   "source": [
    "## 12. Run full RAG pipeline (retrieve + answer)\n",
    "query = \"How is the market?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get answer with context\n",
    "answer, context_chunks = answer_query_with_context(query, top_k=3)\n",
    "\n",
    "print(\"\\nðŸ“„ Retrieved Context (Top 3 chunks):\")\n",
    "print(\"-\" * 60)\n",
    "for i, chunk in enumerate(context_chunks, 1):\n",
    "    print(f\"\\n{i}. {chunk[:200]}...\" if len(chunk) > 200 else f\"\\n{i}. {chunk}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nðŸ¤– AI Answer:\")\n",
    "print(\"-\" * 60)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
